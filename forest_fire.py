# -*- coding: utf-8 -*-
"""forest_fire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8Vp_fZTPD56FBaXjxPAOd7TnFFPSqIo
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Replace 'forest_fire_data.csv' with your dataset file
data = pd.read_csv('/content/forestfires.csv')

# Preview the dataset
print(data.head())

data.columns

# Data preprocessing
X = data[['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH','wind', 'rain']]
y= data['area']

X

y.describe()

"""Random **Forest**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Replace 'forest_fire_data.csv' with your dataset file
data = pd.read_csv('/content/forestfires.csv')

# Preview the dataset
print(data.head())

# Convert 'month' and 'day' columns from text to numeric using one-hot encoding
data = pd.get_dummies(data, columns=['month', 'day'], drop_first=True)

# Data preprocessing: Define features and target variable
X = data[['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain'] + [col for col in data.columns if 'month_' in col or 'day_' in col]]
y = data['area']
#y = np.log1p(data['area'])  # Use log(1 + area) to handle zeros


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
#y_pred = np.expm1(model.predict(X_test))


# Evaluate the model
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}")
print(f"R-squared: {r2_score(y_test, y_pred)}")

# Calculate accuracy within a defined threshold (e.g., ±15)
threshold = 50
correct_predictions = np.abs(y_pred - y_test) <= threshold
accuracy = correct_predictions.mean() * 100  # Convert to percentage

print(f"Accuracy within ±{threshold}: {accuracy:.2f}%")

# Function to determine alert level based on predicted severity
def alert_level(area):
    if area < 10:
        return "Low"
    elif 10 <= area < 50:
        return "Medium"
    else:
        return "High"

# Predicting alert levels for the test set
alert_levels = [alert_level(area) for area in y_pred]

# Display predicted alert levels len(alert_levels)
for i in range(5):
    print(f"Prediction: {alert_levels[i]} (Predicted Area: {y_pred[i]:.2f} hectares)")

# Visualizing feature importance
importances = model.feature_importances_
features = X.columns
indices = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.title('Feature Importances')
plt.barh(range(len(importances)), importances[indices], align='center')
plt.yticks(range(len(importances)), features[indices])
plt.xlabel('Relative Importance')
plt.show()

import pickle

# Assuming 'model' is your trained model
with open('forest_fire_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

import pandas as pd
import pickle

# Load the saved model
with open('forest_fire_model.pkl', 'rb') as model_file:
    saved_model = pickle.load(model_file)

# Retrieve the feature names used during model training
if hasattr(saved_model, 'feature_names_in_'):
    trained_columns = list(saved_model.feature_names_in_)
else:
    raise ValueError("The model does not store the feature names. Ensure you have the correct feature names.")

# Print the actual feature names used during training
print("Trained Columns:", trained_columns)

# Function to generate input data for prediction
def create_input_data(base_data, month_index, day_index):
    month_data = [0] * 12  # for 12 months
    day_data = [0] * 6     # for 6 days (excluding 'day_fri')
    month_data[month_index - 10] = 1
    day_data[day_index - 22] = 1
    return base_data + month_data + day_data

# Base input values for the provided data
base_input = [7, 5, 86.2, 26.2, 94.3, 5.1, 8.2, 51, 6.7, 0.0]  # Basic features

# Generate multiple sets of input data
input_data_list = [
    create_input_data(base_input, trained_columns.index('month_mar'), trained_columns.index('day_thu')),
    create_input_data(base_input, trained_columns.index('month_aug'), trained_columns.index('day_mon')),
    create_input_data(base_input, trained_columns.index('month_dec'), trained_columns.index('day_sat')),
    create_input_data(base_input, trained_columns.index('month_feb'), trained_columns.index('day_sun')),
    create_input_data(base_input, trained_columns.index('month_jul'), trained_columns.index('day_wed'))
]

# Ensure the length of input data matches the model's 27 features
input_data_list = [data[:27] for data in input_data_list]

# Make predictions and determine alert levels
for i, input_data in enumerate(input_data_list):
    if len(input_data) != len(trained_columns):
        raise ValueError(f"Input data length ({len(input_data)}) does not match the number of features in the model ({len(trained_columns)}).")

    input_df = pd.DataFrame([input_data], columns=trained_columns)
    predicted_area = saved_model.predict(input_df)[0]

    def alert_level(area):
        if area < 10:
            return "Low"
        elif 10 <= area < 50:
            return "Medium"
        else:
            return "High"

    predicted_alert = alert_level(predicted_area)

    # Output the results for each input
    #print(f"Prediction {i+1}:")
    #print(f"Predicted Area: {predicted_area}")
    #print(f"Alert Level: {predicted_alert}\n")
    print(f"Predicted Alert Level: {predicted_alert} (Predicted Area: {predicted_area:.2f} hectares)")

import pandas as pd
import pickle
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score



# For demonstration, let's create some mock data
import numpy as np

# Mock data for example (replace this with your actual data)
np.random.seed(42)
X = pd.DataFrame(np.random.rand(100, len(trained_columns)), columns=trained_columns)  # 100 samples
y = np.random.rand(100) * 100  # Random target variable

# Initialize the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
mae_scores = -cv_scores  # Convert to positive MAE

# Print mean and std deviation of the scores
print(f'Mean MAE: {mae_scores.mean():.2f} ± {mae_scores.std():.2f}')

# Plotting the accuracy scores
plt.figure(figsize=(10, 5))
plt.bar(range(1, len(mae_scores) + 1), mae_scores, color='skyblue')
plt.xlabel('Cross-Validation Fold')
plt.ylabel('Mean Absolute Error (MAE)')
plt.title('Random Forest Regressor: MAE Across Cross-Validation Folds')
plt.xticks(range(1, len(mae_scores) + 1))
plt.axhline(mae_scores.mean(), color='red', linestyle='--', label='Mean MAE')
plt.legend()
plt.show()

"""**XGBRegressor**"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the XGBoost Regressor
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
print(f"Mean Squared Error (XGBoost): {mean_squared_error(y_test, y_pred_xgb)}")
print(f"R-squared (XGBoost): {r2_score(y_test, y_pred_xgb)}")

# Make predictions on the test set
y_pred_xgb = xgb_model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred_xgb)
print(f"Mean Squared Error (XGBoost): {mse:.2f}")

# Calculate Mean Absolute Error
mae = mean_absolute_error(y_test, y_pred_xgb)
print(f"Mean Absolute Error (XGBoost): {mae:.2f}")

# Calculate R-squared
r2 = r2_score(y_test, y_pred_xgb)
print(f"R-squared (XGBoost): {r2:.2f}")

# For accuracy, define a threshold for correctness
threshold = 40  # Example threshold - adjust as needed

# Create binary predictions based on the threshold
y_pred_binary = [1 if abs(pred - actual) <= threshold else 0
                   for pred, actual in zip(y_pred_xgb, y_test)]

# Assume all actual values are 'correct' for this accuracy calculation
y_test_binary = [1] * len(y_test)

# Calculate accuracy
accuracy = accuracy_score(y_test_binary, y_pred_binary)
print(f"Accuracy (within threshold {threshold}): {accuracy:.2f}")

import matplotlib.pyplot as plt

# Scatter plot of actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_xgb, alpha=0.6)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (XGBoost)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.show()

# Function to determine alert level based on predicted area
def alert_level(area):
    if area < 10:
        return "Low"
    elif 10 <= area < 50:
        return "Medium"
    else:
        return "High"

# Test the model with alert levels
alert_levels_pred = [alert_level(area) for area in y_pred_xgb]

# Display the alert levels for the first few predictions
for i in range(5):  # Show the first 5 examples
    print(f"Predicted Alert Level: {alert_levels_pred[i]} (Predicted Area: {y_pred_xgb[i]:.2f} hectares)")

"""**LightGBM**"""

# Install LightGBM if not already installed
!pip install lightgbm

import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Assuming data is preprocessed, and X_train, X_test, y_train, y_test are defined

# Create LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test)

# Set hyperparameters for LightGBM
params = {
    'objective': 'regression',
    'metric': 'rmse',  # You can change the metric to mae, etc.
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9
}

# Train the LightGBM model
num_round = 100  # Number of boosting rounds
model_lgb = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# Make predictions
y_pred_lgb = model_lgb.predict(X_test)

# Evaluate the model
mse_lgb = mean_squared_error(y_test, y_pred_lgb)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)

print(f"Mean Squared Error (LightGBM): {mse_lgb}")
print(f"Mean Absolute Error (LightGBM): {mae_lgb}")
print(f"R-squared (LightGBM): {r2_lgb}")

# Calculate accuracy within a defined threshold (e.g., ±15)
threshold = 15
correct_predictions = np.abs(y_pred_lgb - y_test) <= threshold
accuracy = correct_predictions.mean() * 100  # Convert to percentage

print(f"Accuracy within ±{threshold}: {accuracy:.2f}%")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_lgb, color='green', alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)  # Perfect prediction line
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (LightGBM)')
plt.show()


# Feature Importance
lgb.plot_importance(model_lgb, max_num_features=10)
plt.show()

"""**SVR**"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

# Assuming your preprocessed dataset is stored in 'data' DataFrame
# and the target variable is 'area'.
# Splitting the data into features and target
X = data.drop('area', axis=1)
y = (data['area'])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- SVM Model ---
# Use SVR for regression tasks
svm_model = SVR(kernel='rbf')  # Example with an RBF kernel
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# --- Evaluation ---
# Calculate MSE
mse = mean_squared_error(y_test, y_pred_svm)
print(f"Mean Squared Error (MSE): {mse:.2f}")

# Calculate R-squared
r2 = r2_score(y_test, y_pred_svm)
print(f"R-squared (R2): {r2:.2f}")

# For accuracy, you need to define a threshold for correctness.
# For example, consider predictions within a certain range as correct.
threshold = 40  # Example threshold
y_pred_binary = [1 if abs(pred - actual) <= threshold else 0 for pred, actual in zip(y_pred_svm, y_test)]
y_test_binary = [1] * len(y_test)  # All actual values are considered 'correct' in this example

accuracy = accuracy_score(y_test_binary, y_pred_binary)
print(f"Accuracy (within threshold {threshold}): {accuracy:.2f}")

# Visualize actual vs predicted values (Scatter plot)
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_svm, color='blue', alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)  # Perfect prediction line
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (SVR)')
plt.show()

"""**KNN**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# ... (Your existing code for data loading, preprocessing, and train-test split)

# Initialize the KNN Regressor
knn_model = KNeighborsRegressor(n_neighbors=5)  # You can adjust n_neighbors

# Train the model
knn_model.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model
mse_knn = mean_squared_error(y_test, y_pred_knn)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

# Print evaluation metrics
print(f"Mean Squared Error (KNN): {mse_knn}")
print(f"Mean Absolute Error (KNN): {mae_knn}")
print(f"R-squared (KNN): {r2_knn}")

# Calculate a pseudo-accuracy for regression by measuring how many predictions fall within a certain tolerance
tolerance = 40  # Define a tolerance range for considering a prediction "accurate"
accurate_preds = np.abs(y_pred_knn - y_test) <= tolerance
accuracy = np.mean(accurate_preds) * 100
print(f"Pseudo-Accuracy within ±{tolerance}: {accuracy:.2f}%")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_knn, color='purple', alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)  # Perfect prediction line
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (KNN)')
plt.show()

# Import necessary libraries
import pandas as pd
import numpy as np

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning libraries
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# XGBoost and LightGBM
import xgboost as xgb
import lightgbm as lgb

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')


data = pd.read_csv('forestfires.csv')

# Display first few rows
print("First five rows of the dataset:")
print(data.head())

# One-hot encode 'month' and 'day' columns
data = pd.get_dummies(data, columns=['month', 'day'], drop_first=True)

# Define features and target variable
feature_cols = ['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain'] + \
              [col for col in data.columns if 'month_' in col or 'day_' in col]

X = data[feature_cols]
y = data['area']

# Split the dataset into training and testing sets
# You can adjust test_size and random_state as needed
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------------------------
# 2. Model Training and Evaluation
# ---------------------------

# Initialize all regression models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(
        n_estimators=300,       # Increased number of trees
        max_depth=30,           # Increased maximum depth
        min_samples_split=10,    # Increased minimum samples required to split an internal node
        min_samples_leaf=5,     # Increased minimum samples required to be at a leaf node
        random_state=42
    ),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5,
                                random_state=42, verbosity=0),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05, num_leaves=31,
                                  random_state=42),
    'SVR': SVR(kernel='rbf')
}

# Dictionary to store evaluation metrics
evaluation_metrics = {
    'Model': [],
    'MSE': [],
    'MAE': [],
    'R2': [],
    'Accuracy (%)': []
}

# Dictionary to store predictions
predictions = {}

# Train each model and evaluate
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    predictions[name] = y_pred
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    # Calculate Accuracy (%) based on MSE and variance
    variance = np.var(y_test)
    accuracy = (1 - mse / variance) * 100 if variance != 0 else 0
    evaluation_metrics['Model'].append(name)
    evaluation_metrics['MSE'].append(mse)
    evaluation_metrics['MAE'].append(mae)
    evaluation_metrics['R2'].append(r2)
    evaluation_metrics['Accuracy (%)'].append(accuracy)
    print(f"{name} - MSE: {mse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}, Accuracy: {accuracy:.2f}%\n")

# Convert evaluation metrics to DataFrame
metrics_df = pd.DataFrame(evaluation_metrics)
print("Evaluation Metrics:")
print(metrics_df)

# ---------------------------
# 3. Visualization
# ---------------------------

# Set plot style
sns.set(style="whitegrid")

# 3.1 Bar Chart of Accuracy (%)
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Accuracy (%)', data=metrics_df, palette='viridis')
plt.title('Regression Models Accuracy Comparison')
plt.ylabel('Accuracy (%)')
plt.xlabel('Model')
plt.xticks(rotation=45)
plt.ylim(0, 110)  # Set y-axis from 0 to 100+
for index, row in metrics_df.iterrows():
    plt.text(index, row['Accuracy (%)'] + 1, f"{row['Accuracy (%)']:.2f}%", ha='center')
plt.show()

# 3.2 Scatter Plot: Actual vs. Predicted (Top 2 Models by Accuracy)
top_models = metrics_df.sort_values(by='Accuracy (%)', ascending=False).head(2)['Model'].tolist()
plt.figure(figsize=(10, 6))
colors = ['blue', 'red']
for idx, name in enumerate(top_models):
    sns.scatterplot(x=y_test, y=predictions[name], label=name, alpha=0.6, color=colors[idx])
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values (Top 2 Models)')
plt.legend()
plt.show()

# 3.3 Box Plot of Prediction Errors
errors_df = pd.DataFrame()
for name in models.keys():
    errors_df[name] = y_test - predictions[name]

plt.figure(figsize=(12, 8))
sns.boxplot(data=errors_df, palette='Set3')
plt.title('Box Plot of Prediction Errors by Model')
plt.ylabel('Error (Actual - Predicted)')
plt.xticks(rotation=45)
plt.show()

# ---------------------------
# 4. Additional Insights
# ---------------------------

# Example: Accuracy within a Threshold
threshold = 40  # Define your threshold

accuracy_threshold_results = {
    'Model': [],
    f'Accuracy within ±{threshold}': []
}

for name in models.keys():
    correct_predictions = np.abs(predictions[name] - y_test) <= threshold
    accuracy_within_threshold = (correct_predictions.sum() / len(y_test)) * 100
    accuracy_threshold_results['Model'].append(name)
    accuracy_threshold_results[f'Accuracy within ±{threshold}'].append(accuracy_within_threshold)

accuracy_threshold_df = pd.DataFrame(accuracy_threshold_results)
print(f"\nAccuracy comparitions")
print(accuracy_threshold_df)

# Bar chart for accuracy within threshold
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y=f'Accuracy within ±{threshold}', data=accuracy_threshold_df, palette='coolwarm')
plt.title(f'Accuracy comparitions')
plt.ylabel('Accuracy (%)')
plt.xlabel('Model')
plt.xticks(rotation=45)
plt.ylim(0, 100)
for index, row in accuracy_threshold_df.iterrows():
    plt.text(index, row[f'Accuracy within ±{threshold}'] + 1, f"{row[f'Accuracy within ±{threshold}']:.2f}%", ha='center')
plt.show()